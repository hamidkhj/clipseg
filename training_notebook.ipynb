{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arguments:\n",
    "  def __init__(self):\n",
    "    self.batch_size = 1\n",
    "    self.gpu_id = 0\n",
    "    self.train = False\n",
    "    self.checkpoint_dir = \"outputs\"\n",
    "    self.text_prompt = ''\n",
    "    self.output_dir = \"outputs\"\n",
    "    self.dataset_name = \"sample\"\n",
    "    self.train_data_dir = None\n",
    "    self.val_data_dir = None\n",
    "    self.test_data_dir = None\n",
    "    self.optimizer = \"Adam\"\n",
    "    self.epochs = 200\n",
    "    self.lr = 0.1\n",
    "    self.patch_threshold = 100\n",
    "    self.test_mask_size = 512\n",
    "    self.save_test_predictions = True\n",
    "    self.dice_coef = 10\n",
    "    self.boundary_coef = 0.1\n",
    "    self.focal_coef = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import colorsys\n",
    "import cv2\n",
    "\n",
    "def get_random_crop_coordinates(crop_scale_range, image_width, image_height):\n",
    "    rand_number = random.random()\n",
    "    rand_number *= crop_scale_range[1] - crop_scale_range[0]\n",
    "    rand_number += crop_scale_range[0]\n",
    "    patch_size = int(rand_number * min(image_width, image_height))\n",
    "    if patch_size != min(image_width, image_height):\n",
    "        x_start = random.randint(0, image_width - patch_size)\n",
    "        y_start = random.randint(0, image_height - patch_size)\n",
    "    else:\n",
    "        x_start = 0\n",
    "        y_start = 0\n",
    "    return x_start, x_start + patch_size, y_start, y_start + patch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_crops_coords(image_size, patch_size, num_patchs_per_side):\n",
    "    h, w = image_size\n",
    "    if num_patchs_per_side == 1:\n",
    "        x_step_size = y_step_size = 0\n",
    "    else:\n",
    "        x_step_size = (w - patch_size) // (num_patchs_per_side - 1)\n",
    "        y_step_size = (h - patch_size) // (num_patchs_per_side - 1)\n",
    "    crops_coords = []\n",
    "    for i in range(num_patchs_per_side):\n",
    "        for j in range(num_patchs_per_side):\n",
    "            y_start, y_end, x_start, x_end = (\n",
    "                i * y_step_size,\n",
    "                i * y_step_size + patch_size,\n",
    "                j * x_step_size,\n",
    "                j * x_step_size + patch_size,\n",
    "            )\n",
    "            crops_coords.append([y_start, y_end, x_start, x_end])\n",
    "    return crops_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_distinct_colors(n):\n",
    "    colors = []\n",
    "    if n == 1:\n",
    "        return [(255, 255, 255)]\n",
    "    for i in range(n):\n",
    "        hue = i / n\n",
    "        saturation = 0.9\n",
    "        value = 0.9\n",
    "        rgb = colorsys.hsv_to_rgb(hue, saturation, value)\n",
    "        scaled_rgb = tuple(int(x * 255) for x in rgb)\n",
    "        colors.append(scaled_rgb)\n",
    "    return colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iou(prediction, mask):\n",
    "    intersection = prediction * mask\n",
    "    union = prediction + mask - intersection\n",
    "    return intersection.sum() / (union.sum() + 1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_colored_segmentation(mask, boundry_mask, image, colors):\n",
    "    boundry_mask_rgb = 0\n",
    "    if boundry_mask is not None:\n",
    "        boundry_mask_rgb = torch.repeat_interleave(boundry_mask[None, ...], 3, 0).type(\n",
    "            torch.float\n",
    "        )\n",
    "        for j in range(3):\n",
    "            for i in range(1, len(colors) + 1):\n",
    "                boundry_mask_rgb[j] = torch.where(\n",
    "                    boundry_mask_rgb[j] == i,\n",
    "                    colors[i - 1][j] / 255,\n",
    "                    boundry_mask_rgb[j],\n",
    "                )\n",
    "    mask_rgb = torch.repeat_interleave(mask[None, ...], 3, 0).type(torch.float)\n",
    "    for j in range(3):\n",
    "        for i in range(1, len(colors) + 1):\n",
    "            mask_rgb[j] = torch.where(\n",
    "                mask_rgb[j] == i, colors[i - 1][j] / 255, mask_rgb[j]\n",
    "            )\n",
    "    if boundry_mask is not None:\n",
    "        return (boundry_mask_rgb * 0.6 + mask_rgb * 0.3 + image * 0.4).permute(1, 2, 0)\n",
    "    else:\n",
    "        return (mask_rgb * 0.6 + image * 0.4).permute(1, 2, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datalaoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Program Files\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "INFO:albumentations.check_version:A new version of Albumentations is available: 2.0.5 (you have 1.4.8). Upgrade using: pip install --upgrade albumentations\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset as TorchDataset\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import os\n",
    "import random\n",
    "\n",
    "\n",
    "# def get_random_crop_coordinates(crop_ratio_range, width, height):\n",
    "#     \"\"\"Generate random crop coordinates based on the given ratio range.\"\"\"\n",
    "#     crop_ratio = random.uniform(crop_ratio_range[0], crop_ratio_range[1])\n",
    "#     crop_width = int(width * crop_ratio)\n",
    "#     crop_height = int(height * crop_ratio)\n",
    "    \n",
    "#     x_start = random.randint(0, width - crop_width)\n",
    "#     x_end = x_start + crop_width\n",
    "#     y_start = random.randint(0, height - crop_height)\n",
    "#     y_end = y_start + crop_height\n",
    "    \n",
    "#     return x_start, x_end, y_start, y_end\n",
    "\n",
    "\n",
    "class Dataset(TorchDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir,\n",
    "        train=True,\n",
    "        mask_size=352,\n",
    "        num_parts=1,\n",
    "        min_crop_ratio=0.5,\n",
    "        dataset_name: str = \"sample\",\n",
    "    ):\n",
    "        self.image_paths = sorted(glob(os.path.join(data_dir, \"*.png\")))\n",
    "        self.mask_paths = sorted(glob(os.path.join(data_dir, \"*.npy\")))\n",
    "        self.train = train\n",
    "        self.mask_size = mask_size\n",
    "        self.num_parts = num_parts\n",
    "        self.min_crop_ratio = min_crop_ratio\n",
    "        self.current_part_idx = 0\n",
    "        \n",
    "        # Normalize transform (similar to CLIP normalization)\n",
    "        self.normalize_transform = A.Compose([\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "        \n",
    "        # For rotation angle range\n",
    "        if dataset_name == \"celeba\":\n",
    "            rotation_range = (-10, 10)\n",
    "        else:\n",
    "            rotation_range = (-30, 30)\n",
    "            \n",
    "        # Train transforms\n",
    "        self.train_transform_1 = A.Compose([\n",
    "            A.Resize(352, 352),\n",
    "            A.HorizontalFlip(),\n",
    "            A.GaussianBlur(blur_limit=(3, 5))\n",
    "        ])\n",
    "        \n",
    "        self.train_transform_2 = A.Compose([\n",
    "            A.Resize(352, 352),\n",
    "            A.Rotate(\n",
    "                rotation_range,\n",
    "                border_mode=cv2.BORDER_CONSTANT,\n",
    "                value=0,\n",
    "                mask_value=0\n",
    "            )\n",
    "        ])\n",
    "        \n",
    "        # Test transform\n",
    "        self.test_transform = A.Compose([\n",
    "            A.Resize(352, 352),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load the image\n",
    "        image = np.array(Image.open(self.image_paths[idx]))\n",
    "        \n",
    "        if self.train:\n",
    "            # Load mask\n",
    "            mask = np.load(self.mask_paths[idx])\n",
    "            \n",
    "            # Apply first set of transforms (resize, flip, blur)\n",
    "            result = self.train_transform_1(image=image, mask=mask)\n",
    "            image, mask = result[\"image\"], result[\"mask\"]\n",
    "            \n",
    "            # Calculate original mask size for current part\n",
    "            original_mask_size = np.where(mask == self.current_part_idx, 1, 0).sum()\n",
    "            \n",
    "            # Random crop ensuring mask is included\n",
    "            mask_is_included = False\n",
    "            while not mask_is_included:\n",
    "                x_start, x_end, y_start, y_end = get_random_crop_coordinates(\n",
    "                    (self.min_crop_ratio, 1), 352, 352\n",
    "                )\n",
    "                aux_mask = mask[y_start:y_end, x_start:x_end]\n",
    "                if (\n",
    "                    original_mask_size == 0\n",
    "                    or np.where(aux_mask == self.current_part_idx, 1, 0).sum() / original_mask_size > 0.3\n",
    "                ):\n",
    "                    mask_is_included = True\n",
    "            \n",
    "            image = image[y_start:y_end, x_start:x_end]\n",
    "            mask = aux_mask\n",
    "            \n",
    "            # Apply second set of transforms (rotation)\n",
    "            result = self.train_transform_2(image=image, mask=mask)\n",
    "            image, mask = result[\"image\"], result[\"mask\"]\n",
    "            \n",
    "            # Convert to tensor and normalize\n",
    "            result = self.normalize_transform(image=image)\n",
    "            image = result[\"image\"]\n",
    "            \n",
    "            # Convert mask to tensor\n",
    "            mask = torch.from_numpy(mask).float()\n",
    "            \n",
    "            # Update current part index\n",
    "            self.current_part_idx += 1\n",
    "            self.current_part_idx = self.current_part_idx % self.num_parts\n",
    "            \n",
    "            return image, mask\n",
    "        else:\n",
    "            # Test mode\n",
    "            if len(self.mask_paths) > 0:\n",
    "                mask = np.load(self.mask_paths[idx])\n",
    "                result = self.test_transform(image=image, mask=mask)\n",
    "                image, mask = result[\"image\"], result[\"mask\"]\n",
    "                mask = torch.from_numpy(mask).float()\n",
    "            else:\n",
    "                result = self.test_transform(image=image)\n",
    "                image = result[\"image\"]\n",
    "                mask = torch.zeros((352, 352))\n",
    "                \n",
    "            return image, mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "\n",
    "class DataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_data_dir: str = \"./data\",\n",
    "        val_data_dir: str = \"./data\",\n",
    "        test_data_dir: str = \"./data\",\n",
    "        batch_size: int = 1,\n",
    "        train_mask_size: int = 352,\n",
    "        test_mask_size: int = 352,\n",
    "        num_parts: int = 2,\n",
    "        min_crop_ratio: float = 0.5,\n",
    "        dataset_name: str = \"sample\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.train_data_dir = train_data_dir\n",
    "        self.val_data_dir = val_data_dir\n",
    "        self.test_data_dir = test_data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.train_mask_size = train_mask_size\n",
    "        self.test_mask_size = test_mask_size\n",
    "        self.num_parts = num_parts\n",
    "        self.min_crop_ratio = min_crop_ratio\n",
    "        self.dataset_name = dataset_name\n",
    "\n",
    "    def setup(self, stage: str):\n",
    "        if stage == \"fit\":\n",
    "            self.train_dataset = Dataset(\n",
    "                data_dir=self.train_data_dir,\n",
    "                train=True,\n",
    "                mask_size=self.train_mask_size,\n",
    "                num_parts=self.num_parts,\n",
    "                min_crop_ratio=self.min_crop_ratio,\n",
    "                dataset_name=self.dataset_name,\n",
    "            )\n",
    "            self.val_dataset = Dataset(\n",
    "                data_dir=self.val_data_dir,\n",
    "                train=False,\n",
    "                mask_size=self.test_mask_size,\n",
    "            )\n",
    "        elif stage == \"test\":\n",
    "            self.test_dataset = Dataset(\n",
    "                data_dir=self.test_data_dir,\n",
    "                train=False,\n",
    "                mask_size=self.test_mask_size,\n",
    "            )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset, batch_size=self.batch_size, num_workers=0, shuffle=True\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_dataset, batch_size=self.batch_size, num_workers=0, shuffle=False\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_dataset, batch_size=self.batch_size, num_workers=0, shuffle=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# custom loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# class DiceLoss(nn.Module):\n",
    "#     def __init__(self, smooth=1.0):\n",
    "#         super(DiceLoss, self).__init__()\n",
    "#         self.smooth = smooth\n",
    "        \n",
    "#     def forward(self, predictions, targets):\n",
    "#         \"\"\"\n",
    "#         Compute Dice loss between predictions and target masks\n",
    "        \n",
    "#         Args:\n",
    "#             predictions: Tensor of shape (B, H, W) or (H, W), predicted probabilities\n",
    "#             targets: Binary mask of shape (B, H, W) or (H, W)\n",
    "            \n",
    "#         Returns:\n",
    "#             Dice loss (1 - Dice coefficient)\n",
    "#         \"\"\"\n",
    "#         # Flatten the tensors\n",
    "#         predictions = predictions.view(-1)\n",
    "#         targets = targets.view(-1)\n",
    "        \n",
    "#         # Calculate intersection and union\n",
    "#         intersection = (predictions * targets).sum()\n",
    "#         union = predictions.sum() + targets.sum()\n",
    "        \n",
    "#         # Calculate Dice coefficient\n",
    "#         dice = (2. * intersection + self.smooth) / (union + self.smooth)\n",
    "        \n",
    "#         # Return Dice loss\n",
    "#         return 1 - dice\n",
    "\n",
    "\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1.0):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "        \n",
    "    def forward(self, predictions, targets):\n",
    "        # Ensure shapes match\n",
    "        if predictions.shape != targets.shape:\n",
    "            targets = targets.view(predictions.shape)\n",
    "            \n",
    "        # Flatten prediction and target tensors\n",
    "        predictions = predictions.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        intersection = (predictions * targets).sum()\n",
    "        dice = (2. * intersection + self.smooth) / (\n",
    "            predictions.sum() + targets.sum() + self.smooth\n",
    "        )\n",
    "        \n",
    "        return 1 - dice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ClipSeg@@@@"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "import gc\n",
    "from PIL import Image\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "# load the model\n",
    "\n",
    "import torch\n",
    "import requests\n",
    "\n",
    "from models.clipseg import CLIPDensePredT\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class CLIPSeg22(pl.LightningModule):\n",
    "    def __init__(self, config, learning_rate=0.001):\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = config\n",
    "        # self.save_hyperparameters(config.__dict__)\n",
    "        self.max_val_iou = 0\n",
    "        self.val_ious = []\n",
    "\n",
    "        # self.device1 = 'cpu'\n",
    "        self.device1 = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "        # load model\n",
    "        self.model = CLIPDensePredT(version='ViT-B/16', reduce_dim=64)\n",
    "        self.model.train()\n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        # non-strict, because we only stored decoder weights (not CLIP weights)\n",
    "        self.model.load_state_dict(torch.load('clipseg_weights/rd64-uni.pth', map_location=torch.device('cpu')), strict=False)\n",
    "\n",
    "        self.model = self.model.to(self.device1)\n",
    "\n",
    "        base_prompt = ''\n",
    "\n",
    "        import clip\n",
    "\n",
    "        # text_tokens = clip.tokenize(base_prompt).to(self.device1)\n",
    "        # emb = self.model.clip_model.encode_text(text_tokens)\n",
    "        # self.emb_to_learn = emb.detach().clone().to(self.device1)  # Detach and clone to make it a leaf tensor\n",
    "        # self.emb_to_learn.requires_grad_(True)  # Correct way to set requires_grad\n",
    "\n",
    "        # Create embedding properly as a model parameter\n",
    "        with torch.no_grad():\n",
    "            text_tokens = clip.tokenize(base_prompt).to(self.device1)\n",
    "            emb = self.model.clip_model.encode_text(text_tokens)\n",
    "            self.emb_to_learn = torch.nn.Parameter(emb.clone())\n",
    "\n",
    "\n",
    "\n",
    "    def on_fit_start(self) -> None: #pl: called at the beginning of the fit()\n",
    "        # move model to gpu\n",
    "        pass\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        image, mask = batch\n",
    "        image = image.to(self.device1)\n",
    "        mask = mask.to(self.device1)\n",
    "        \n",
    "        # print('training step was called..........')\n",
    "        # print(image.shape)\n",
    "        \n",
    "        # Get model predictions\n",
    "        # print('self emb to learn is : ', self.emb_to_learn.shape)\n",
    "        preds = self.model(image, self.emb_to_learn)[0]\n",
    "        # print('preds grad: ', preds.requires_grad)\n",
    "        \n",
    "        # Apply sigmoid but keep the computational graph intact\n",
    "        prediction = torch.sigmoid(preds)\n",
    "        \n",
    "        # print('prediction shape: ', prediction.shape)\n",
    "        # print('mask shape :', mask.shape)\n",
    "        \n",
    "        # Make sure mask and prediction have compatible shapes\n",
    "        # Assuming mask is [B, H, W] and prediction is [B, 1, H, W]\n",
    "        if len(mask.shape) < len(prediction.shape):\n",
    "            mask = mask.unsqueeze(1)\n",
    "        \n",
    "        # Use DiceLoss\n",
    "        dice = DiceLoss().to(self.device1)\n",
    "        loss = dice(prediction, mask)\n",
    "        # loss = F.cross_entropy(\n",
    "        #     prediction,\n",
    "        #     mask.type(torch.float16),\n",
    "        # )\n",
    "        # print('loss is : ', loss)\n",
    "\n",
    "        # if self.emb_to_learn.grad is not None:\n",
    "        #     print(\"Embedding grad in training step:\", \n",
    "        #         self.emb_to_learn.grad.min().item(),\n",
    "        #         self.emb_to_learn.grad.mean().item(), \n",
    "        #         self.emb_to_learn.grad.max().item())\n",
    "        # else:\n",
    "        #     print(\"Embedding grad is None in training step\")\n",
    "        \n",
    "        # Log the loss if needed\n",
    "        # self.log(\"train_loss\", loss, on_step=True, prog_bar=True)\n",
    "        # print('mean is : lkals;dafkj: ',self.emb_to_learn.mean())\n",
    "        \n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def on_validation_start(self):\n",
    "        pass\n",
    "\n",
    "    def on_validation_epoch_start(self):\n",
    "        self.val_ious = []\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        image, mask = batch\n",
    "        #calculate mask / iou\n",
    "\n",
    "        # self.log(\"val mean iou\", mean_iou.cpu(), on_step=True, sync_dist=True)\n",
    "        return torch.tensor(0.0)\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        # epoch_mean_iou = sum(self.val_ious) / len(self.val_ious)\n",
    "        # if epoch_mean_iou >= self.max_val_iou:\n",
    "        #     self.max_val_iou = epoch_mean_iou\n",
    "        #     for i, embedding in enumerate(self.emb_to_learn): #save embeddings\n",
    "        #         torch.save(\n",
    "        #             embedding,\n",
    "        #             os.path.join(self.checkpoint_dir, f\"embedding_{i}.pth\"),\n",
    "        #         )\n",
    "        gc.collect() #python garbage collector\n",
    "\n",
    "    def on_test_start(self) -> None:\n",
    "        # make directory for saving results\n",
    "        # load the embeddings if needed\n",
    "        pass\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # Saving just the emb_to_learn parameter\n",
    "        torch.save(self.emb_to_learn, 'emb_to_learn.pt')\n",
    "\n",
    "\n",
    "        # do your test visualize/iou/save results/log results\n",
    "        test_pth = 'datasets/dataset/test'\n",
    "        print('--------------------------------------------------------------')\n",
    "        img_list = [x for x in os.listdir(test_pth) if '.png' in x]\n",
    "        print(len(img_list))\n",
    "\n",
    "        print('--------------------------------------------------------------')\n",
    "        for img_name in img_list:\n",
    "            input_image = Image.open(f'{test_pth}/{img_name}')\n",
    "            mask = np.load(f\"{test_pth}/{img_name.replace('.png', '.npy')}\")\n",
    "\n",
    "\n",
    "            transform = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                transforms.Resize((352, 352)),\n",
    "            ])\n",
    "\n",
    "            img = transform(input_image).unsqueeze(0)\n",
    "\n",
    "\n",
    "            # predict\n",
    "            with torch.no_grad():\n",
    "                preds = self.model(img, self.emb_to_learn, is_training=False)[0]\n",
    "\n",
    "            #mask generation\n",
    "            # filename = f\"mask.png\"\n",
    "            # here we save the second mask\n",
    "            m = torch.sigmoid(preds[0][0]).cpu()\n",
    "            plt.imsave(f'results/{img_name}',m)\n",
    "\n",
    "\n",
    "            # Create binary mask\n",
    "            img2 = cv2.imread(f'results/{img_name}')\n",
    "            gray_image = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
    "            (thresh, bw_image) = cv2.threshold(gray_image, 150, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "            # Save the binary mask using cv2.imwrite instead of plt.imsave\n",
    "            bmask_filename = f\"results/{img_name.replace('.png', '')}_bmask.png\"\n",
    "            cv2.imwrite(bmask_filename, bw_image)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # img2 = cv2.imread(f'results/{img_name}')\n",
    "            # gray_image = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            # (thresh, bw_image) = cv2.threshold(gray_image, 150, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "            # # fix color format\n",
    "            # cv2.cvtColor(bw_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            # # Image.fromarray(bw_image)\n",
    "\n",
    "            # # visualize prediction\n",
    "            # _, ax = plt.subplots(1, 4, figsize=(10, 4))\n",
    "            # [a.axis('off') for a in ax.flatten()]\n",
    "            # ax[0].imshow(input_image)\n",
    "            # ax[1].imshow(m)\n",
    "            # ax[1].text(0, -15, 'polyp')\n",
    "            # ax[2].imshow(bw_image)\n",
    "            # ax[3].imshow(mask, cmap='gray')\n",
    "\n",
    "\n",
    "        return torch.tensor(0.0)\n",
    "\n",
    "    def on_test_end(self) -> None:\n",
    "        print(\"max val mean iou: \", self.max_val_iou)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = getattr(optim, self.config.optimizer)(\n",
    "            [self.emb_to_learn],  # Pass the parameter directly\n",
    "            lr=self.config.lr,\n",
    "        )\n",
    "        return optimizer\n",
    "\n",
    "    def on_before_optimizer_step(self, optimizer):\n",
    "        if self.trainer.global_step % 1 == 0:  # Avoid excessive logging\n",
    "            for param in self.emb_to_learn:\n",
    "                if param.grad is None:\n",
    "                    print( \"is None, requires_grad : \", param.requires_grad)\n",
    "                else:\n",
    "                    print ( \"grad\", param.grad.data.max(), param.grad.data.min())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is None, requires_grad :  True\n",
      "Epoch 499: 100%|██████████| 11/11 [00:00<00:00, 30.17it/s, v_num=79]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=500` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 499: 100%|██████████| 11/11 [00:00<00:00, 30.17it/s, v_num=79]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA GeForce RTX 4070 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "e:\\Program Files\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Program Files\\Python310\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------\n",
      "681\n",
      "--------------------------------------------------------------\n",
      "Testing DataLoader 0: 100%|██████████| 1/1 [00:56<00:00, 56.29s/it]\n",
      "max val mean iou:  0\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "\n",
    "def main():\n",
    "    config = Arguments()\n",
    "    config.dataset_name = \"pascal\"\n",
    "    config.train_data_dir = \"datasets/dataset/train\"\n",
    "    config.val_data_dir = \"\"\n",
    "    config.test_data_dir = \"\"\n",
    "    config.train = True\n",
    "    config.epochs = 500\n",
    "\n",
    "\n",
    "    dm = DataModule(\n",
    "        train_data_dir=config.train_data_dir,\n",
    "        val_data_dir=config.val_data_dir,\n",
    "        test_data_dir=config.test_data_dir,\n",
    "        batch_size=config.batch_size,\n",
    "        test_mask_size=config.test_mask_size,\n",
    "        dataset_name=config.dataset_name,\n",
    "    )\n",
    "    model = CLIPSeg22(config=config)\n",
    "    if isinstance(config.gpu_id, int):\n",
    "        gpu_id = [config.gpu_id]\n",
    "    else:\n",
    "        gpu_id = config.gpu_id\n",
    "    trainer = pl.Trainer(\n",
    "        # accelerator=\"gpu\",\n",
    "        default_root_dir=config.output_dir,\n",
    "        max_epochs=config.epochs,\n",
    "        devices=gpu_id,\n",
    "        log_every_n_steps=1,\n",
    "        enable_checkpointing=False,\n",
    "        num_sanity_val_steps=0,\n",
    "    )\n",
    "    if config.train:\n",
    "        trainer.fit(model=model, datamodule=dm)\n",
    "        trainer.test(model=model, datamodule=dm)\n",
    "    else:\n",
    "        trainer.test(model=model, datamodule=dm)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Later, to load it:\n",
    "# loaded_emb = torch.load('emb_to_learn.pt')\n",
    "# self.emb_to_learn = torch.nn.Parameter(loaded_emb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
